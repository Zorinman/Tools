# WebContentExtractor 快速使用指南

## 🎯 5分钟上手

### 第一步：准备环境

确保已安装Python和依赖库：
```bash
pip install requests beautifulsoup4
```

### 第二步：创建提取脚本

创建一个新的Python文件，例如 `my_extraction.py`：

```python
from WebContentExtractor import WebContentExtractor, ExtractionConfig

# 配置
config = ExtractionConfig(
    base_url="https://你的网站.com",
    output_dir="提取的文章",
    main_content_selector="main",  # 根据网站调整
)

# 文章列表
articles = [
    {"title": "文章标题1", "url": "完整URL1"},
    {"title": "文章标题2", "url": "完整URL2"},
]

# 执行提取
extractor = WebContentExtractor(config)
results = extractor.extract_articles(articles)
```

### 第三步：运行

```bash
python my_extraction.py
```

---

## 🔍 如何找到正确的选择器？

### 方法1：使用浏览器开发者工具

1. 打开要提取的网页
2. 按 F12 打开开发者工具
3. 点击左上角的"选择元素"图标
4. 点击文章主体内容区域
5. 在开发者工具中查看高亮的HTML元素

**常见的主内容选择器**：
- `main` - 标签名
- `article` - 标签名
- `.content` - class名
- `#main-content` - id
- `div.post-content` - 组合选择器

### 方法2：查看网页源代码

右键点击网页 → "查看网页源代码"，搜索文章标题，找到包含主要内容的容器元素。

---

## 📝 实战案例

### 案例1：提取CSDN博客

```python
from WebContentExtractor import WebContentExtractor, ExtractionConfig

config = ExtractionConfig(
    base_url="https://blog.csdn.net",
    output_dir="CSDN文章",
    main_content_selector="article",  # CSDN使用article标签
    title_selector="h1.title-article",
    skip_selectors=['nav', 'aside', '.comment-list'],
    download_images=True,
)

articles = [
    {
        "title": "Python爬虫教程",
        "url": "https://blog.csdn.net/xxx/article/details/12345"
    }
]

extractor = WebContentExtractor(config)
extractor.extract_articles(articles)
```

### 案例2：提取简书文章

```python
config = ExtractionConfig(
    base_url="https://www.jianshu.com",
    output_dir="简书文章",
    main_content_selector="article",
    title_selector="h1._1RuRku",
    skip_selectors=['nav', 'aside', '._2-bzBq'],
    download_images=True,
)
```

### 案例3：批量提取（从文件读取）

**1. 创建文章列表文件 `articles.json`**：
```json
[
  {
    "title": "文章1",
    "url": "https://example.com/post1"
  },
  {
    "title": "文章2", 
    "url": "https://example.com/post2"
  }
]
```

**2. 提取脚本**：
```python
import json
from WebContentExtractor import WebContentExtractor, ExtractionConfig

# 读取文章列表
with open('articles.json', 'r', encoding='utf-8') as f:
    articles = json.load(f)

# 配置并提取
config = ExtractionConfig(
    base_url="https://example.com",
    output_dir="批量文章",
)

extractor = WebContentExtractor(config)
extractor.extract_articles(articles)
```

---

## 🛠️ 常见配置调整

### 只提取文本，不下载图片

```python
config = ExtractionConfig(
    download_images=False,  # 关闭图片下载
    # ... 其他配置
)
```

### 慢速爬取（避免被封IP）

```python
config = ExtractionConfig(
    delay=3.0,  # 每篇文章间隔3秒
    timeout=60,  # 超时时间60秒
    # ... 其他配置
)
```

### 自定义请求头（模拟浏览器）

```python
config = ExtractionConfig(
    headers={
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Referer': 'https://example.com',
        'Cookie': '你的Cookie'  # 如果需要登录
    },
    # ... 其他配置
)
```

### 跳过特定元素

```python
config = ExtractionConfig(
    skip_selectors=[
        'nav',           # 导航栏
        'aside',         # 侧边栏
        'footer',        # 页脚
        '.comments',     # 评论区
        '#ads',          # 广告
        '.related-posts' # 相关文章推荐
    ],
    # ... 其他配置
)
```

---

## ⚡ 性能优化技巧

### 1. 分批处理大量文章

```python
# 将大列表分成小批次
def batch_extract(all_articles, batch_size=10):
    for i in range(0, len(all_articles), batch_size):
        batch = all_articles[i:i+batch_size]
        print(f"处理第 {i//batch_size + 1} 批，共 {len(batch)} 篇")
        extractor.extract_articles(batch)
        time.sleep(10)  # 批次间休息
```

### 2. 先测试再批量

```python
# 先用1-2篇测试配置
test_articles = articles[:2]
extractor.extract_articles(test_articles)

# 确认配置正确后再处理全部
# extractor.extract_articles(articles)
```

### 3. 关闭不需要的功能

```python
config = ExtractionConfig(
    download_images=False,  # 不下载图片
    create_index=False,     # 不创建索引
    save_failed_urls=False, # 不保存失败列表
    verbose=False,          # 不输出详细信息
)
```

---

## 🔧 故障诊断

### 问题：找不到内容

**检查清单**：
1. ✅ `base_url` 是否正确？
2. ✅ `main_content_selector` 是否匹配网页结构？
3. ✅ 网页是否需要登录才能查看？
4. ✅ 是否被反爬虫机制拦截？

**解决方法**：
```python
# 开启详细输出
config.verbose = True

# 测试选择器
from bs4 import BeautifulSoup
import requests

response = requests.get("测试URL")
soup = BeautifulSoup(response.text, 'html.parser')
main = soup.select_one("你的选择器")
print(main)  # 查看是否找到内容
```

### 问题：格式不正确

**可能原因**：
- 没有跳过不需要的元素（导航、侧边栏等）
- 选择器匹配了错误的区域

**解决方法**：
```python
# 增加要跳过的选择器
config.skip_selectors = [
    'nav', 'aside', 'footer',
    '.sidebar', '.comments', '.ads'
]
```

### 问题：图片下载失败

**解决方法**：
```python
# 临时关闭图片下载
config.download_images = False

# 或增加超时时间
config.timeout = 60
```

---

## 📱 获取帮助

如有问题，请查看：
1. **完整文档**: `README.md`
2. **使用示例**: `example_usage.py`
3. **配置说明**: `config.py`

---

## ✅ 使用检查清单

开始提取前，确认：

- [ ] 已安装依赖：`pip install requests beautifulsoup4`
- [ ] 已设置正确的 `base_url`
- [ ] 已找到正确的 `main_content_selector`
- [ ] 已准备好文章列表（title + url）
- [ ] 已测试1-2篇文章确认配置正确
- [ ] 已设置合理的 `delay` 避免被封
- [ ] 已了解目标网站的使用条款

提取完成后，记得：

- [ ] 删除临时创建的提取脚本（如 `extract_xxx.py`）
- [ ] 删除 `__pycache__` 目录（Python编译缓存）
- [ ] 仅保留提取的文章内容目录

**祝使用愉快！** 🎉
