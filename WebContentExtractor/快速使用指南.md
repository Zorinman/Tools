# WebContentExtractor å¿«é€Ÿä½¿ç”¨æŒ‡å—

## ğŸ¯ 5åˆ†é’Ÿä¸Šæ‰‹

### ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡ç¯å¢ƒ

ç¡®ä¿å·²å®‰è£…Pythonå’Œä¾èµ–åº“ï¼š
```bash
pip install requests beautifulsoup4
```

### ç¬¬äºŒæ­¥ï¼šåˆ›å»ºæå–è„šæœ¬

åˆ›å»ºä¸€ä¸ªæ–°çš„Pythonæ–‡ä»¶ï¼Œä¾‹å¦‚ `my_extraction.py`ï¼š

```python
from WebContentExtractor import WebContentExtractor, ExtractionConfig

# é…ç½®
config = ExtractionConfig(
    base_url="https://ä½ çš„ç½‘ç«™.com",
    output_dir="æå–çš„æ–‡ç« ",
    main_content_selector="main",  # æ ¹æ®ç½‘ç«™è°ƒæ•´
)

# æ–‡ç« åˆ—è¡¨
articles = [
    {"title": "æ–‡ç« æ ‡é¢˜1", "url": "å®Œæ•´URL1"},
    {"title": "æ–‡ç« æ ‡é¢˜2", "url": "å®Œæ•´URL2"},
]

# æ‰§è¡Œæå–
extractor = WebContentExtractor(config)
results = extractor.extract_articles(articles)
```

### ç¬¬ä¸‰æ­¥ï¼šè¿è¡Œ

```bash
python my_extraction.py
```

---

## ğŸ” å¦‚ä½•æ‰¾åˆ°æ­£ç¡®çš„é€‰æ‹©å™¨ï¼Ÿ

### æ–¹æ³•1ï¼šä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·

1. æ‰“å¼€è¦æå–çš„ç½‘é¡µ
2. æŒ‰ F12 æ‰“å¼€å¼€å‘è€…å·¥å…·
3. ç‚¹å‡»å·¦ä¸Šè§’çš„"é€‰æ‹©å…ƒç´ "å›¾æ ‡
4. ç‚¹å‡»æ–‡ç« ä¸»ä½“å†…å®¹åŒºåŸŸ
5. åœ¨å¼€å‘è€…å·¥å…·ä¸­æŸ¥çœ‹é«˜äº®çš„HTMLå…ƒç´ 

**å¸¸è§çš„ä¸»å†…å®¹é€‰æ‹©å™¨**ï¼š
- `main` - æ ‡ç­¾å
- `article` - æ ‡ç­¾å
- `.content` - classå
- `#main-content` - id
- `div.post-content` - ç»„åˆé€‰æ‹©å™¨

### æ–¹æ³•2ï¼šæŸ¥çœ‹ç½‘é¡µæºä»£ç 

å³é”®ç‚¹å‡»ç½‘é¡µ â†’ "æŸ¥çœ‹ç½‘é¡µæºä»£ç "ï¼Œæœç´¢æ–‡ç« æ ‡é¢˜ï¼Œæ‰¾åˆ°åŒ…å«ä¸»è¦å†…å®¹çš„å®¹å™¨å…ƒç´ ã€‚

---

## ğŸ“ å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šæå–CSDNåšå®¢

```python
from WebContentExtractor import WebContentExtractor, ExtractionConfig

config = ExtractionConfig(
    base_url="https://blog.csdn.net",
    output_dir="CSDNæ–‡ç« ",
    main_content_selector="article",  # CSDNä½¿ç”¨articleæ ‡ç­¾
    title_selector="h1.title-article",
    skip_selectors=['nav', 'aside', '.comment-list'],
    download_images=True,
)

articles = [
    {
        "title": "Pythonçˆ¬è™«æ•™ç¨‹",
        "url": "https://blog.csdn.net/xxx/article/details/12345"
    }
]

extractor = WebContentExtractor(config)
extractor.extract_articles(articles)
```

### æ¡ˆä¾‹2ï¼šæå–ç®€ä¹¦æ–‡ç« 

```python
config = ExtractionConfig(
    base_url="https://www.jianshu.com",
    output_dir="ç®€ä¹¦æ–‡ç« ",
    main_content_selector="article",
    title_selector="h1._1RuRku",
    skip_selectors=['nav', 'aside', '._2-bzBq'],
    download_images=True,
)
```

### æ¡ˆä¾‹3ï¼šæ‰¹é‡æå–ï¼ˆä»æ–‡ä»¶è¯»å–ï¼‰

**1. åˆ›å»ºæ–‡ç« åˆ—è¡¨æ–‡ä»¶ `articles.json`**ï¼š
```json
[
  {
    "title": "æ–‡ç« 1",
    "url": "https://example.com/post1"
  },
  {
    "title": "æ–‡ç« 2", 
    "url": "https://example.com/post2"
  }
]
```

**2. æå–è„šæœ¬**ï¼š
```python
import json
from WebContentExtractor import WebContentExtractor, ExtractionConfig

# è¯»å–æ–‡ç« åˆ—è¡¨
with open('articles.json', 'r', encoding='utf-8') as f:
    articles = json.load(f)

# é…ç½®å¹¶æå–
config = ExtractionConfig(
    base_url="https://example.com",
    output_dir="æ‰¹é‡æ–‡ç« ",
)

extractor = WebContentExtractor(config)
extractor.extract_articles(articles)
```

---

## ğŸ› ï¸ å¸¸è§é…ç½®è°ƒæ•´

### åªæå–æ–‡æœ¬ï¼Œä¸ä¸‹è½½å›¾ç‰‡

```python
config = ExtractionConfig(
    download_images=False,  # å…³é—­å›¾ç‰‡ä¸‹è½½
    # ... å…¶ä»–é…ç½®
)
```

### æ…¢é€Ÿçˆ¬å–ï¼ˆé¿å…è¢«å°IPï¼‰

```python
config = ExtractionConfig(
    delay=3.0,  # æ¯ç¯‡æ–‡ç« é—´éš”3ç§’
    timeout=60,  # è¶…æ—¶æ—¶é—´60ç§’
    # ... å…¶ä»–é…ç½®
)
```

### è‡ªå®šä¹‰è¯·æ±‚å¤´ï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨ï¼‰

```python
config = ExtractionConfig(
    headers={
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Referer': 'https://example.com',
        'Cookie': 'ä½ çš„Cookie'  # å¦‚æœéœ€è¦ç™»å½•
    },
    # ... å…¶ä»–é…ç½®
)
```

### è·³è¿‡ç‰¹å®šå…ƒç´ 

```python
config = ExtractionConfig(
    skip_selectors=[
        'nav',           # å¯¼èˆªæ 
        'aside',         # ä¾§è¾¹æ 
        'footer',        # é¡µè„š
        '.comments',     # è¯„è®ºåŒº
        '#ads',          # å¹¿å‘Š
        '.related-posts' # ç›¸å…³æ–‡ç« æ¨è
    ],
    # ... å…¶ä»–é…ç½®
)
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. åˆ†æ‰¹å¤„ç†å¤§é‡æ–‡ç« 

```python
# å°†å¤§åˆ—è¡¨åˆ†æˆå°æ‰¹æ¬¡
def batch_extract(all_articles, batch_size=10):
    for i in range(0, len(all_articles), batch_size):
        batch = all_articles[i:i+batch_size]
        print(f"å¤„ç†ç¬¬ {i//batch_size + 1} æ‰¹ï¼Œå…± {len(batch)} ç¯‡")
        extractor.extract_articles(batch)
        time.sleep(10)  # æ‰¹æ¬¡é—´ä¼‘æ¯
```

### 2. å…ˆæµ‹è¯•å†æ‰¹é‡

```python
# å…ˆç”¨1-2ç¯‡æµ‹è¯•é…ç½®
test_articles = articles[:2]
extractor.extract_articles(test_articles)

# ç¡®è®¤é…ç½®æ­£ç¡®åå†å¤„ç†å…¨éƒ¨
# extractor.extract_articles(articles)
```

### 3. å…³é—­ä¸éœ€è¦çš„åŠŸèƒ½

```python
config = ExtractionConfig(
    download_images=False,  # ä¸ä¸‹è½½å›¾ç‰‡
    create_index=False,     # ä¸åˆ›å»ºç´¢å¼•
    save_failed_urls=False, # ä¸ä¿å­˜å¤±è´¥åˆ—è¡¨
    verbose=False,          # ä¸è¾“å‡ºè¯¦ç»†ä¿¡æ¯
)
```

---

## ğŸ”§ æ•…éšœè¯Šæ–­

### é—®é¢˜ï¼šæ‰¾ä¸åˆ°å†…å®¹

**æ£€æŸ¥æ¸…å•**ï¼š
1. âœ… `base_url` æ˜¯å¦æ­£ç¡®ï¼Ÿ
2. âœ… `main_content_selector` æ˜¯å¦åŒ¹é…ç½‘é¡µç»“æ„ï¼Ÿ
3. âœ… ç½‘é¡µæ˜¯å¦éœ€è¦ç™»å½•æ‰èƒ½æŸ¥çœ‹ï¼Ÿ
4. âœ… æ˜¯å¦è¢«åçˆ¬è™«æœºåˆ¶æ‹¦æˆªï¼Ÿ

**è§£å†³æ–¹æ³•**ï¼š
```python
# å¼€å¯è¯¦ç»†è¾“å‡º
config.verbose = True

# æµ‹è¯•é€‰æ‹©å™¨
from bs4 import BeautifulSoup
import requests

response = requests.get("æµ‹è¯•URL")
soup = BeautifulSoup(response.text, 'html.parser')
main = soup.select_one("ä½ çš„é€‰æ‹©å™¨")
print(main)  # æŸ¥çœ‹æ˜¯å¦æ‰¾åˆ°å†…å®¹
```

### é—®é¢˜ï¼šæ ¼å¼ä¸æ­£ç¡®

**å¯èƒ½åŸå› **ï¼š
- æ²¡æœ‰è·³è¿‡ä¸éœ€è¦çš„å…ƒç´ ï¼ˆå¯¼èˆªã€ä¾§è¾¹æ ç­‰ï¼‰
- é€‰æ‹©å™¨åŒ¹é…äº†é”™è¯¯çš„åŒºåŸŸ

**è§£å†³æ–¹æ³•**ï¼š
```python
# å¢åŠ è¦è·³è¿‡çš„é€‰æ‹©å™¨
config.skip_selectors = [
    'nav', 'aside', 'footer',
    '.sidebar', '.comments', '.ads'
]
```

### é—®é¢˜ï¼šå›¾ç‰‡ä¸‹è½½å¤±è´¥

**è§£å†³æ–¹æ³•**ï¼š
```python
# ä¸´æ—¶å…³é—­å›¾ç‰‡ä¸‹è½½
config.download_images = False

# æˆ–å¢åŠ è¶…æ—¶æ—¶é—´
config.timeout = 60
```

---

## ğŸ“± è·å–å¸®åŠ©

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ï¼š
1. **å®Œæ•´æ–‡æ¡£**: `README.md`
2. **ä½¿ç”¨ç¤ºä¾‹**: `example_usage.py`
3. **é…ç½®è¯´æ˜**: `config.py`

---

## âœ… ä½¿ç”¨æ£€æŸ¥æ¸…å•

å¼€å§‹æå–å‰ï¼Œç¡®è®¤ï¼š

- [ ] å·²å®‰è£…ä¾èµ–ï¼š`pip install requests beautifulsoup4`
- [ ] å·²è®¾ç½®æ­£ç¡®çš„ `base_url`
- [ ] å·²æ‰¾åˆ°æ­£ç¡®çš„ `main_content_selector`
- [ ] å·²å‡†å¤‡å¥½æ–‡ç« åˆ—è¡¨ï¼ˆtitle + urlï¼‰
- [ ] å·²æµ‹è¯•1-2ç¯‡æ–‡ç« ç¡®è®¤é…ç½®æ­£ç¡®
- [ ] å·²è®¾ç½®åˆç†çš„ `delay` é¿å…è¢«å°
- [ ] å·²äº†è§£ç›®æ ‡ç½‘ç«™çš„ä½¿ç”¨æ¡æ¬¾

æå–å®Œæˆåï¼Œè®°å¾—ï¼š

- [ ] åˆ é™¤ä¸´æ—¶åˆ›å»ºçš„æå–è„šæœ¬ï¼ˆå¦‚ `extract_xxx.py`ï¼‰
- [ ] åˆ é™¤ `__pycache__` ç›®å½•ï¼ˆPythonç¼–è¯‘ç¼“å­˜ï¼‰
- [ ] ä»…ä¿ç•™æå–çš„æ–‡ç« å†…å®¹ç›®å½•

**ç¥ä½¿ç”¨æ„‰å¿«ï¼** ğŸ‰
